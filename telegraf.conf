[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s.
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  precision = ""

  ## Log at debug level.
  debug = true
  ## Log only error level messages.
  quiet = false

  ## Log target controls the destination for logs and can be one of "file",
  ## "stderr" or, on Windows, "eventlog".  When set to "file", the output file
  ## is determined by the "logfile" setting.
  # logtarget = "file"

  ## Name of the file to be logged to when using the "file" logtarget.  If set to
  ## the empty string then logs are written to stderr.
  # logfile = ""

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0d"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  # logfile_rotation_max_size = "0MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  # logfile_rotation_max_archives = 5

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  # log_with_timezone = ""

  ## Override default hostname, if empty use os.Hostname()
  #hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  #omit_hostname = false

# Starlark Processor Plugin
[[processors.starlark]]
  # Inline Starlark script
  source = '''
def apply(metric):
    # Create a dictionary for the new structure
    new_metric = {
        'customer': '',
        'group': '',
        'device': '',
        'utility_type': '',
        'panel_board': '',
        'power_analyze': '',
        'parameter': [],
        # Initialize additional fields
        'i_avg': 0.0,
        'v_avg': 0.0,
        'P': 0.0,
        'pf': 0.0,
        'ap_total': 0.0,
        'f': 0.0,
        'E': 0.0,
        'vA_rms': 0.0,
        'vB_rms': 0.0,
        'vC_rms': 0.0
    }

    # Iterate through fields in the original metric
    for key, value in metric.fields.items():
        # Split the key to get the different parts
        parts = key.split('/')
        if len(parts) >= 7:
            # Assign values to the new structure
            new_metric['customer'] = str(parts[0])
            new_metric['group'] = str(parts[1])
            new_metric['device'] = str(parts[2])
            new_metric['utility_type'] = str(parts[3])
            new_metric['panel_board'] = str(parts[4])
            new_metric['power_analyze'] = str(parts[5])

            # Assign specific fields or add to parameter array
            field_key = parts[6]
            #print("@@@@@@@@@@@@@ field_key", field_key)
            #print("@@@@@@@@@@@@@ value", value)
            if field_key in new_metric:
                new_metric[field_key] = value  # Keep numeric value
            else:
                new_metric['parameter'].append({'key': field_key, 'val': value})  # Keep numeric value in parameters

    # Convert the new structure into a metric
    for k, v in new_metric.items():
        if k == 'parameter':
            metric.fields['parameters'] = str(v)
        else:
            # Ensure all tags are converted to strings
            metric.tags[k] = str(v)
    return metric
  '''


[[outputs.influxdb_v2]]
  ## The URLs of the InfluxDB cluster nodes.
  ##
  ## Multiple URLs can be specified for a single cluster, only ONE of the
  ## urls will be written to each interval.
  ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
  urls = ["http://influxdb:8086"]

  ## Token for authentication.
  token = "4eYvsu8wZCJ6tKuE2sxvFHkvYFwSMVK0011hEEiojvejzpSaij86vYQomN_12au6eK-2MZ6Knr-Sax201y70w=="

  ## Organization is the name of the organization you wish to write to; must exist.
  organization = "BRN"

  ## Destination bucket to write into.
  bucket = "BLI"

  ## The value of this tag will be used to determine the bucket.  If this
  ## tag is not set the 'bucket' option is used as the default.
  # bucket_tag = ""

  ## If true, the bucket tag will not be added to the metric.
  # exclude_bucket_tag = false

  ## Timeout for HTTP messages.
  # timeout = "5s"

  ## Additional HTTP headers
  # http_headers = {"X-Special-Header" = "Special-Value"}

  ## HTTP Proxy override, if unset values the standard proxy environment
  ## variables are consulted to determine which proxy, if any, should be used.
  # http_proxy = "http://corporate.proxy:3128"

  ## HTTP User-Agent
  # user_agent = "telegraf"

  ## Content-Encoding for write request body, can be set to "gzip" to
  ## compress body or "identity" to apply no encoding.
  # content_encoding = "gzip"

  ## Enable or disable uint support for writing uints influxdb 2.0.
  # influx_uint_support = false

  ## Optional TLS Config for use on HTTP connections.
  # tls_ca = "/etc/telegraf/ca.pem"
  # tls_cert = "/etc/telegraf/cert.pem"
  # tls_key = "/etc/telegraf/key.pem"
  ## Use TLS but skip chain & host verification
  # insecure_skip_verify = false

# # Send telegraf metrics to file(s)
[[outputs.file]]
#   ## Files to write to, "stdout" is a specially handled file.
    files = ["stdout"]
#
#   ## The file will be rotated after the time interval specified.  When set
#   ## to 0 no time based rotation is performed.
#   # rotation_interval = "0d"
#
#   ## The logfile will be rotated when it becomes larger than the specified
#   ## size.  When set to 0 no size based rotation is performed.
#   # rotation_max_size = "0MB"
#
#   ## Maximum number of rotated archives to keep, any older logs are deleted.
#   ## If set to -1, no archives are removed.
#   # rotation_max_archives = 5
#
#   ## Data format to output.
#   ## Each data format has its own unique set of configuration options, read
#   ## more about them here:
#   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
#   data_format = "influx"

[[inputs.mqtt_consumer]]
  servers = ["tcp://mosquitto:1883"]
  topics = [
    "BRN/BLI/#"
  ]
  data_format = "json"

#   [[inputs.exec]]
#   commands = ["/etc/telegraf/script.sh"]
#   name_override = "systemnameX"
#   timeout = "5s"
#   data_format = "value"
#   data_type = "string"
#   [inputs.exec.tags]
#   type = "ip_address"





#   [[inputs.mqtt_consumer.topic_parsing]]
#       data_format = "value"
#       data_type = "float"
#       topic = "BRN/BLI/BLIW12/elec/solar/solar"
#       measurement = "measurement/_/_/_/_/_"
#       tags = "customer/group/plant/utility_type/panel_board/power_analyzer"
#       fields = "_/_/_/_/_/temperature"
#       [[processors.pivot]]
#       tag_key = "field"
#       value_key = "value"


  #json_query = "{humidity,temperature,battery_voltage_mv}"
  #json_name_key = "dev_id"
  #tag_keys = ["dev_id", "hardware_serial"]
  #json_string_fields=["hardware_serial"]
  #[tags]
  #  source="mqtt"
